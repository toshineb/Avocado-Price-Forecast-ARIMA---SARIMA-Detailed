{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/toshineb/Avocado-Price-Forecast-ARIMA---SARIMA-Detailed/blob/main/Avocado_Price_Forecast_ARIMA_SARIMA_%7C_Detailed.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUK9M_Quyu62"
      },
      "source": [
        "# <center>Avocado Price Forecasting</center>\n",
        "\n",
        "### Aim :\n",
        "- To predict / forecast the average price of Avocado based on time series data.\n",
        "- It is a typical **Time Series Analysis** problem.\n",
        "\n",
        "### <center>Dataset Attributes</center>\n",
        "    \n",
        "- **Date** :The date of the observation\n",
        "- **AveragePrice** : the average price of a single avocado\n",
        "- **Total Volume** : Total number of avocados sold\n",
        "- **4046** : Total number of avocados with PLU 4046 sold\n",
        "- **4225** : Total number of avocados with PLU 4225 sold\n",
        "- **4770** : Total number of avocados with PLU 4770 sold\n",
        "- **Total Bags** : Total number of bags\n",
        "- **Small Bags** : Total number of small bags\n",
        "- **Large Bags** : Total number of large bags\n",
        "- **XLarge Bags** : Extra Large Bags\n",
        "- **type** : conventional or organic\n",
        "- **year** : year of the date\n",
        "- **region** : the city or region of the observation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8SjA1nlyu64"
      },
      "source": [
        "### Notebook Contents :\n",
        "- Dataset Information\n",
        "- Exploratory Data Analysis (EDA)\n",
        "- Summary of EDA\n",
        "- Time Series Analysis\n",
        "- Modeling\n",
        "- Conclusion\n",
        "\n",
        "### What you will learn :\n",
        "- Statistical Tests for Time Series Analysis.\n",
        "- Order selection for ARIMA & SARIMA models.\n",
        "- In-sample and Out-of-sample forecasting using rolling and non-rolling methods.\n",
        "- Difference between forecast function and predict function of ARIMA & SARIMA.\n",
        "\n",
        "\n",
        "### Lets get started!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WULTILE9yu66"
      },
      "source": [
        "### Import the Necessary Libraries :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vlsZu7aayu66"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "\n",
        "from itertools import combinations\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "from statsmodels.tsa.stattools import acf, pacf\n",
        "from statsmodels.tsa.arima_model import ARIMA as ARIMA\n",
        "import statsmodels.api as sm\n",
        "import statsmodels.tsa.api as smt\n",
        "pd.options.display.float_format = '{:.2f}'.format"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('/content/avocado.csv')\n",
        "data = data.drop('Unnamed: 0',axis = 1)\n",
        "data.head()"
      ],
      "metadata": {
        "id": "gnun65e9__fz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvGj-cYEyu68"
      },
      "source": [
        "### Data Info :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eq4Jnpvryu68"
      },
      "outputs": [],
      "source": [
        "data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gKUsqKK7yu69"
      },
      "outputs": [],
      "source": [
        "data.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PyukFSR7yu6-"
      },
      "outputs": [],
      "source": [
        "data.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcRNwQUKyu6-"
      },
      "source": [
        "- **No null values** present in the data!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DG5u5LOmyu6-"
      },
      "outputs": [],
      "source": [
        "data.describe().T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Boi6lx5yu6_"
      },
      "outputs": [],
      "source": [
        "df1 = data.copy(deep = True)\n",
        "df1['Date'] = pd.to_datetime(df1['Date'])\n",
        "df1.set_index('Date',inplace = True)\n",
        "df1 = df1.resample('W').sum()\n",
        "df1.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8AgEQFxyu6_"
      },
      "source": [
        "- I will repeat and explain about these steps in detail below. Till then, we will just use this for visualization if required."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7Xpen67yu7A"
      },
      "source": [
        "# <center>Exploratory Data Analysis</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0WHhRHnyu7A"
      },
      "source": [
        "### Target Variable Visualization (AveragePrice) :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sd1xoDgnyu7A"
      },
      "outputs": [],
      "source": [
        "color1 = ['#296C92','#3EB489']\n",
        "fig,ax = plt.subplots(nrows = 1, ncols = 2,figsize = (15,5))\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "sns.distplot(data['AveragePrice']);\n",
        "plt.title('Distribution : AveragePrice')\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "sns.distplot(df1['AveragePrice']);\n",
        "plt.title('Distribution : AveragePrice')\n",
        "\n",
        "fig,ax = plt.subplots(nrows = 1, ncols = 1,figsize = (20,5))\n",
        "plt.subplot(1,1,1)\n",
        "sns.lineplot(x = df1.index ,y = 'AveragePrice',data = df1,palette = color1);\n",
        "plt.title('AveragePrice vs Date')\n",
        "\n",
        "plt.show()\n",
        "fig.tight_layout(pad = 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMYLDkcoyu7C"
      },
      "source": [
        "- Distribution of **AveragePrice** that is not resampled is pretty much a **normally distributed** curve. It highlights small double peaks but we will allow it in this case.\n",
        "- Distribution of **AveragePrice** of the resampled data displays a much better **normally distribution** curve.\n",
        "- We can clearly observe a positive trend in **AveragePrice** w.r.t **Date**. Repetitive 3 peaks at consistent intervals of time can be observed.\n",
        "- **AveragePrice** drops around the months of December / January and rises to it's highest value for the months September - November."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KR_oXVQXyu7C"
      },
      "source": [
        "### Dividing features into Numerical and Categorical :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J6k1x16xyu7C"
      },
      "outputs": [],
      "source": [
        "col = list(data.columns)\n",
        "categorical_features = []\n",
        "numerical_features = []\n",
        "for i in col:\n",
        "    if len(data[i].unique()) > 6:\n",
        "        numerical_features.append(i)\n",
        "    else:\n",
        "        categorical_features.append(i)\n",
        "\n",
        "numerical_features.remove('Date')\n",
        "numerical_features.remove('AveragePrice')\n",
        "numerical_features.remove('region')\n",
        "print('Categorical Features :',*categorical_features)\n",
        "print('Numerical Features :',*numerical_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctt-JtSuyu7D"
      },
      "source": [
        "- We remove the **Date** from the list of numerical_features as it's datatype is supposed to be datetime.\n",
        "- **AveragePrice** and **region** are removed as we will visualize them separately from the numerical features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RsS1p1Meyu7D"
      },
      "source": [
        "### Categorical Features :\n",
        "\n",
        "#### Distribution of Categorical Features :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0cFb_7Mmyu7E"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "\n",
        "data['type'] = le.fit_transform(data['type'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VIPl6gUVyu7E"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(nrows = 1,ncols = 2,figsize = (10,5))\n",
        "for i in range(len(categorical_features)):\n",
        "\n",
        "    plt.subplot(1,2,i+1)\n",
        "    sns.distplot(data[categorical_features[i]],kde_kws = {'bw' : 1});\n",
        "    title = 'Distribution : ' + categorical_features[i]\n",
        "    plt.title(title)\n",
        "\n",
        "fig, ax = plt.subplots(nrows = 1,ncols = 2,figsize = (10,5))\n",
        "for i in range(len(categorical_features)):\n",
        "\n",
        "    plt.subplot(1,2,i+1)\n",
        "    sns.countplot(data[categorical_features[i]],palette = color1);\n",
        "    title = 'Count : ' + categorical_features[i]\n",
        "    plt.title(title)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3xYitehyu7F"
      },
      "source": [
        "- Both the features are **Normally Distributed**.\n",
        "- Conventional and Organic **type of Avocados** are present in equal numbers.\n",
        "- Similarly, data points are pretty much same for **year** values 2015, 2016, 2017 and it is followed by a very sharp drop in 2018 probably because of no further data in 2018."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tENvBcGYyu7F"
      },
      "source": [
        "### Categorical Features vs Target Variable (AveragePrice) :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yeh1kki-yu7F"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(nrows = 1,ncols = 2,figsize = (10,5))\n",
        "for i in range(len(categorical_features)):\n",
        "\n",
        "    plt.subplot(1,2,i+1)\n",
        "    sns.barplot(x = categorical_features[i],y = 'AveragePrice',data = data,palette = color1,edgecolor = 'black')\n",
        "    title = categorical_features[i] + ' vs AveragePrice'\n",
        "    plt.title(title);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdcIPFUJyu7G"
      },
      "source": [
        "- **AveragePrice** of Conventional(0) avocados is less than those of Organic(1).\n",
        "- **AveragePrice** of Avocados is near about the same for the **years** 2015, 2016 and 2018. A rise in **AveragePrice** can be clearly seen for the year 2017."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4dbmxgGyu7G"
      },
      "source": [
        "### Numerical Features :\n",
        "\n",
        "#### Distribution of Numerical Features :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nuQ9BCqqyu7H"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(nrows = 4,ncols = 2,figsize = (15,20))\n",
        "for i in range(len(numerical_features)):\n",
        "    plt.subplot(4,2,i+1)\n",
        "    sns.distplot(df1[numerical_features[i]])\n",
        "    title = 'Distribution : ' + numerical_features[i]\n",
        "    plt.title(title)\n",
        "plt.show()\n",
        "fig.tight_layout(h_pad = 10,w_pad = 5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5Bi2Gf5yu7H"
      },
      "source": [
        "- Distributions of the non-resampled data were understandable, hence we visulize the distributions of resampled data.\n",
        "- **Total Volume**, **4046** & **4225** kind of display a **normally distributed** curve. Remaining numerical features display either a **Double Peak Distribution** or **Right / Positive Skewed Distribution**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtduxD1jyu7H"
      },
      "source": [
        "### Numerical Features vs Categorical Features :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpEb5sY3yu7I"
      },
      "source": [
        "#### Numerical Features vs type :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "66AKg4jAyu7I"
      },
      "outputs": [],
      "source": [
        "total_volume = [sum(data[data['type'] == 0]['Total Volume']) / sum(data['Total Volume']) * 100,\n",
        "                sum(data[data['type'] == 1]['Total Volume']) / sum(data['Total Volume']) * 100]\n",
        "\n",
        "avocado_4046 = [sum(data[data['type'] == 0]['4046']) / sum(data['4046']) * 100,\n",
        "                sum(data[data['type'] == 1]['4046']) / sum(data['4046']) * 100]\n",
        "\n",
        "avocado_4225 = [sum(data[data['type'] == 0]['4225']) / sum(data['4225']) * 100,\n",
        "                sum(data[data['type'] == 1]['4225']) / sum(data['4225']) * 100]\n",
        "\n",
        "avocado_4770 = [sum(data[data['type'] == 0]['4770']) / sum(data['4770']) * 100,\n",
        "                sum(data[data['type'] == 1]['4770']) / sum(data['4770']) * 100]\n",
        "\n",
        "total_bags = [sum(data[data['type'] == 0]['Total Bags']) / sum(data['Total Bags']) * 100,\n",
        "              sum(data[data['type'] == 1]['Total Bags']) / sum(data['Total Bags']) * 100]\n",
        "\n",
        "small_bags = [sum(data[data['type'] == 0]['Small Bags']) / sum(data['Small Bags']) * 100,\n",
        "              sum(data[data['type'] == 1]['Small Bags']) / sum(data['Small Bags']) * 100]\n",
        "\n",
        "large_bags = [sum(data[data['type'] == 0]['Large Bags']) / sum(data['Large Bags']) * 100,\n",
        "              sum(data[data['type'] == 1]['Large Bags']) / sum(data['Large Bags']) * 100]\n",
        "\n",
        "xlarge_bags = [sum(data[data['type'] == 0]['XLarge Bags']) / sum(data['XLarge Bags']) * 100,\n",
        "               sum(data[data['type'] == 1]['XLarge Bags']) / sum(data['XLarge Bags']) * 100]\n",
        "\n",
        "type_numerical_features_percentage = [total_volume,avocado_4046,avocado_4225,avocado_4770,\n",
        "                          total_bags,small_bags,large_bags,xlarge_bags]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-crqJQq6yu7J"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(nrows = 4,ncols = 2,figsize = (12,12))\n",
        "\n",
        "for i in range(len(numerical_features)):\n",
        "    plt.subplot(4,2,i + 1)\n",
        "    plt.pie(type_numerical_features_percentage[i],labels = ['Conventional (0)','Organic (1)'],autopct = '%1.1f%%',\n",
        "            startangle = 90, explode = (0,0.1), colors = color1,\n",
        "            wedgeprops = {'edgecolor' : 'black','linewidth': 1,'antialiased' : True})\n",
        "    title = numerical_features[i]\n",
        "    plt.title(title);\n",
        "\n",
        "fig.tight_layout(pad = 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFtDOIZ9yu7J"
      },
      "source": [
        "- Use of **Conventional** avocados dominates the use of **Organic** avocados by a huge margin for all the features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWBzy7lLyu7K"
      },
      "source": [
        "#### Numerical Features vs year :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HJ9BWZutyu7K"
      },
      "outputs": [],
      "source": [
        "total_volume = [sum(data[data['year'] == 2015]['Total Volume']) / sum(data['Total Volume']) * 100,\n",
        "                sum(data[data['year'] == 2016]['Total Volume']) / sum(data['Total Volume']) * 100,\n",
        "                sum(data[data['year'] == 2017]['Total Volume']) / sum(data['Total Volume']) * 100,\n",
        "                sum(data[data['year'] == 2018]['Total Volume']) / sum(data['Total Volume']) * 100]\n",
        "\n",
        "avocado_4046 = [sum(data[data['year'] == 2015]['4046']) / sum(data['4046']) * 100,\n",
        "                sum(data[data['year'] == 2016]['4046']) / sum(data['4046']) * 100,\n",
        "                sum(data[data['year'] == 2017]['4046']) / sum(data['4046']) * 100,\n",
        "                sum(data[data['year'] == 2018]['4046']) / sum(data['4046']) * 100]\n",
        "\n",
        "avocado_4225 = [sum(data[data['year'] == 2015]['4225']) / sum(data['4225']) * 100,\n",
        "                sum(data[data['year'] == 2016]['4225']) / sum(data['4225']) * 100,\n",
        "                sum(data[data['year'] == 2017]['4225']) / sum(data['4225']) * 100,\n",
        "                sum(data[data['year'] == 2018]['4225']) / sum(data['4225']) * 100]\n",
        "\n",
        "avocado_4770 = [sum(data[data['year'] == 2015]['4770']) / sum(data['4770']) * 100,\n",
        "                sum(data[data['year'] == 2016]['4770']) / sum(data['4770']) * 100,\n",
        "                sum(data[data['year'] == 2017]['4770']) / sum(data['4770']) * 100,\n",
        "                sum(data[data['year'] == 2018]['4770']) / sum(data['4770']) * 100]\n",
        "\n",
        "total_bags = [sum(data[data['year'] == 2015]['Total Bags']) / sum(data['Total Bags']) * 100,\n",
        "              sum(data[data['year'] == 2016]['Total Bags']) / sum(data['Total Bags']) * 100,\n",
        "              sum(data[data['year'] == 2017]['Total Bags']) / sum(data['Total Bags']) * 100,\n",
        "              sum(data[data['year'] == 2018]['Total Bags']) / sum(data['Total Bags']) * 100]\n",
        "\n",
        "small_bags = [sum(data[data['year'] == 2015]['Small Bags']) / sum(data['Small Bags']) * 100,\n",
        "              sum(data[data['year'] == 2016]['Small Bags']) / sum(data['Small Bags']) * 100,\n",
        "              sum(data[data['year'] == 2017]['Small Bags']) / sum(data['Small Bags']) * 100,\n",
        "              sum(data[data['year'] == 2018]['Small Bags']) / sum(data['Small Bags']) * 100]\n",
        "\n",
        "large_bags = [sum(data[data['year'] == 2015]['Large Bags']) / sum(data['Large Bags']) * 100,\n",
        "              sum(data[data['year'] == 2016]['Large Bags']) / sum(data['Large Bags']) * 100,\n",
        "              sum(data[data['year'] == 2017]['Large Bags']) / sum(data['Large Bags']) * 100,\n",
        "              sum(data[data['year'] == 2018]['Large Bags']) / sum(data['Large Bags']) * 100]\n",
        "\n",
        "xlarge_bags = [sum(data[data['year'] == 2015]['XLarge Bags']) / sum(data['XLarge Bags']) * 100,\n",
        "               sum(data[data['year'] == 2016]['XLarge Bags']) / sum(data['XLarge Bags']) * 100,\n",
        "               sum(data[data['year'] == 2017]['XLarge Bags']) / sum(data['XLarge Bags']) * 100,\n",
        "               sum(data[data['year'] == 2018]['XLarge Bags']) / sum(data['XLarge Bags']) * 100]\n",
        "\n",
        "year_numerical_features_percentage = [total_volume,avocado_4046,avocado_4225,avocado_4770,\n",
        "                                      total_bags,small_bags,large_bags,xlarge_bags]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a9pB_HKdyu7L"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(nrows = 4,ncols = 2,figsize = (12,12))\n",
        "\n",
        "for i in range(len(numerical_features)):\n",
        "    plt.subplot(4,2,i + 1)\n",
        "    plt.pie(year_numerical_features_percentage[i],labels = ['2015','2016','2017','2018'],autopct = '%1.1f%%',\n",
        "            startangle = 90, explode = (0.1,0,0.1,0.1), colors = color1,\n",
        "            wedgeprops = {'edgecolor' : 'black','linewidth': 1,'antialiased' : True})\n",
        "    title = numerical_features[i]\n",
        "    plt.title(title);\n",
        "fig.tight_layout(pad = 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSh5nErbyu7L"
      },
      "source": [
        "- **Total Volume** of avocados has increased year on year starting from 2015. Values for the year 2018 are low because the data recorded is of the 1st 3 months of 2018.\n",
        "- In 2015, all the 3 types of avocados, **4046**, **4225**, **4770**, are consumed in near about equal amounts. However, in 2016, a clear preference towards **4770** type avocado can be seen with 38.4% people opting for it which is followed by **4225** and **4046** respectively.\n",
        "- This order preference of avocado type is completely flipped in 2017 where **4046** is the most preferred avocado with 30.9% and **4770**'s preference has observed a significant drop to 21.9%. **4225** also experienced drop in its preference by 1.33% on average over the 3 years.\n",
        "- For **bags**, a rising growth in its usage can be seen from 2015. A concrete growth in the usage of **bags** can be observed from 2015 - 2016 by an average margin of **19.07**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xT_A-uZ1yu7L"
      },
      "source": [
        "### Numerical Features vs Date :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHsR4vUzyu7M"
      },
      "source": [
        "- Initially, we will look for the bigger picture of the features : **AveragePrice**, **Total Volume**, **Total Bags**. Then we will proceed to visualize the features derived from the above features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KOxEKTV9yu7M"
      },
      "outputs": [],
      "source": [
        "l1 = ['AveragePrice','Total Volume','Total Bags']\n",
        "fig, ax = plt.subplots(nrows = 3,ncols = 1,figsize = (20,15))\n",
        "for i in range(len(l1)):\n",
        "\n",
        "    plt.subplot(3,1,i+1)\n",
        "    sns.lineplot(x = df1.index,y = l1[i],data = df1)\n",
        "    title = l1[i] + ' vs Date'\n",
        "    plt.title(title);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEswrj-7yu7N"
      },
      "source": [
        "- From the above graphs, we can say that **AveragePrice** vs **Total Volume** & **Total Bags** is negatively correlated or complement each other.\n",
        "- **AveragePrice** is inversely proportional to **Total Volume** & **Total Bags** whereas **Total Volume** displays a directly proportional relationship with **Total Bags**.\n",
        "- Assuming the graphs are superimposed, the crests & troughs of **AveragePrice** would overlap the troughs & crests of **Total Volume** & **Total Bags** respectively.\n",
        "- This relationship between **AveragePrice** vs **Total Volume** & **Total Bags** w.r.t **Date** can be associated with the **Law of Supply and Demand**.\n",
        "    - If the supply increases and demand stays the same, the price will go down.\n",
        "    - If the supply decreases and demand stays the same, the price will go up.\n",
        "    \n",
        "- **AveragePrice**, **Total Volume** and **Total Bags** displays an uptrend with **Date**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cxV5EFU_yu7N"
      },
      "outputs": [],
      "source": [
        "l2 = ['4046','4225','4770','Small Bags','Large Bags','XLarge Bags']\n",
        "\n",
        "fig, ax = plt.subplots(nrows = 3,ncols = 2,figsize = (20,15))\n",
        "for i in range(len(l2)):\n",
        "\n",
        "    plt.subplot(3,2,i+1)\n",
        "    sns.lineplot(x = df1.index,y = l2[i],data = df1)\n",
        "    title = l2[i] + ' vs Date'\n",
        "    plt.title(title);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYYeUdWjyu7N"
      },
      "source": [
        "- Avocados with PLU (Product Lookup Code) **4046** and **4225** kind of display the same patterns w.r.t **Date**. Crests and troughs are very similar to each other.\n",
        "- **4770** type of avocado has a seen a decrease in it's demand as time progresses. It has encountered a sharp drop in demand during the later months of 2016 and has not recovered since then.  \n",
        "- Use of **bags** w.r.t avocados has definitely been on the up! All the **bags** display a rising graph w.r.t **Date**. From the values present on y-axis, customers prefer to use the **Small Bags**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZE81FwDyu7O"
      },
      "source": [
        "### Numerical Features vs region w.r.t Categorical Features :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLarj9Yhyu7O"
      },
      "source": [
        "- In this case, we will only visualize the **Total Volume** and **Total Bags** numerical features against **region** w.r.t **year** and **type**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxaJkbWgyu7O"
      },
      "source": [
        "#### Total Volume vs region w.r.t year :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bb4gXyfxyu7P"
      },
      "outputs": [],
      "source": [
        "color2 = ['#DF3C22','#203EB9', '#F5EE04','#50CD27']\n",
        "\n",
        "sns.catplot(x = 'Total Volume',y = 'region',data = data,hue = 'year',height = 8,palette = color2,kind = 'point');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyjkLZfeyu7P"
      },
      "source": [
        "- This graph is unable to provide significant insights about year on year difference in avocado **Total Volume** consumed in different cities due to overlapping of data points.\n",
        "- **region** has mixed elements with the names of the cities and division of the country based on cardinal direction & non-cardinal directions like : **[Midsouth, Northeast, SouthCentral, Southeast, West, TotalUS]**.\n",
        "- All the elements based on cardinal directions & non-cardinal directions display a huge spike in demand for avocado. This is probably because of the data collected for these elements is a combination of mulitple city data.\n",
        "- **California**, **Great Lakes**, **Los Angeles** and **Plains** are some of the cities that highlight heavy consumption of avocado through **Total Volume**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDaccubTyu7P"
      },
      "source": [
        "#### Total Volume vs region w.r.t type :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O5x-ARwGyu7Q"
      },
      "outputs": [],
      "source": [
        "sns.catplot(x='Total Volume',y='region',data = data,hue = 'type',height = 8,palette = color1,kind = 'point');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2fRqdYQyu7Q"
      },
      "source": [
        "- This graph is very similar to the graph above in terms of the positions of the spike.\n",
        "- Elements based on cardinal directions & non-cardinal directions display the same spikes with a clear cut preference towards **Conventional(0)** avocados that supports the information gained from other visualizations.\n",
        "- Cities can like **DallasFtWorth**, **Denver**, **Houston**, **New York**, **PhoenixTucson** and **San Francisco** can be added to the previous of **California**, **Great Lakes**, **Los Angeles** and **Plains** that showcase their preference towards **Conventional(0)** avocado."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXbcRhtPyu7Q"
      },
      "source": [
        "#### Total Bags vs region w.r.t year :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J9yLl-poyu7R"
      },
      "outputs": [],
      "source": [
        "sns.catplot(x='Total Bags',y='region',data = data,hue = 'year',height = 8,palette = color2,kind = 'point');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-iHqu_tyu7S"
      },
      "source": [
        "- Use of **bags** is highly correlated with **Total Volume**.\n",
        "- From the data point **TotalUS** on the y-axis of the graph, we can say that there is crystal clear rise in use of bags."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPvaT-mQyu7S"
      },
      "source": [
        "#### Total Bags vs region w.r.t type :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kw0iSJxLyu7S"
      },
      "outputs": [],
      "source": [
        "sns.catplot(x='Total Bags',y='region',data = data,hue = 'type',height = 8,palette = color1,kind = 'point');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUKOYCr_yu7T"
      },
      "source": [
        "- This graph is also very similar to the graphs above. No new information was gained."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJgeUBfMyu7T"
      },
      "source": [
        "# <center>Summary of EDA</center>\n",
        "\n",
        "- **AveragePrice** of the avocados starts just below 1.2 and is found to be rising above 1.8 during the peak season. This rise in **AveragePrice** is due to the low **Total Volume** of avocados around the months of December / January and drop in **AveragePrice** is in the months September - November.\n",
        "- When it comes down to the type of avocados, **Conventional(0)** dominates the **Organic(1)** type by a huge margin. **AveragePrice** of **Conventional(0)** type is just below 1.2 and for **Organic(1)** it is above 1.6.\n",
        "- **Law of Supply & Demand** can be observed for **AveragePrice** of avocados vs **Total Volume** & **Total Bags**.\n",
        "- A clear cut rising trend in the **Total Volume** of the avocados can be observed from 2015. Assuming the average purchase order of the avocados is quite high, customers prefer the **Conventional(0)** avocados due to its low **AveragePrice** compared to that of **Organic(1)**.\n",
        "- For avocados based on PLU code **4046**, **4225**, **4770** following order of preference can be observed :\n",
        "    - **2015** : **4046** ≅ **4225** ≅ **4770**\n",
        "    - **2016** : **4770** > **4225** > **4046**\n",
        "    - **2017** : **4046** > **4225** > **4770**\n",
        "- For **bags**, **Law of Cause and Effect** can be observed with **Total Volume** of avocados. As the **Total Volume** of the avocados increased, usage of bags for avocados went up as well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzCZz2-Vyu7T"
      },
      "source": [
        "# <center>Time Series Analysis</center>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y7WsxFhVyu7U"
      },
      "outputs": [],
      "source": [
        "df1 = data.copy(deep = True)\n",
        "df1['Date'] = pd.to_datetime(df1['Date'])\n",
        "df1.set_index('Date',inplace = True)\n",
        "df1 = df1.resample('W').sum()\n",
        "df1.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_mG2upCyu7U"
      },
      "source": [
        "- We create a deep copy of the original dataset and resample the data in this new copy of dataset.\n",
        "- Converting the datatype of the **Date** column to datetime and setting it as the index of the dataset.\n",
        "- In this case, we resample the elements of the **Date** column weekly i.e the data gets arranged in the weekly format. Resampling according to week, reduces the dataset dataset size which was orginally in weekday format.\n",
        "- The aggregated function used in this case is **sum()**. Due to this if any data point falls in between the week, the values of that data point will be added up and it will be assigned to the 1st day of the week.\n",
        "\n",
        "### Before Resampling :\n",
        "        \n",
        "|Date|AveragePrice|\n",
        "|-|-|\n",
        "|2015-01-04|100|\n",
        "|2015-01-05|100|\n",
        "|2015-01-06|100|\n",
        "|2015-01-11|200|\n",
        "\n",
        "### After Resampling :\n",
        "        \n",
        "|Date|AveragePrice|\n",
        "|-|-|\n",
        "|2015-01-04|300|\n",
        "|2015-01-11|200|"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ko7wNiovyu7U"
      },
      "outputs": [],
      "source": [
        "l3 = list(df1.columns)\n",
        "l3.remove('AveragePrice')\n",
        "\n",
        "df1.drop(columns = l3,inplace =True)\n",
        "df1.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlMNLUDYyu7V"
      },
      "source": [
        "- This is a univariate time series analysis problem where we want to forecast a single time dependent variable, **AveragePrice**. Hence, we set **Date** as the dataframe index and drop the remaining columns.\n",
        "- To avoid typing the names of all the columns to be dropped, a list is created with all the columns to be dropped from the columns method of the dataframe and **AveragePrice**, the feature we require, is dropped from the list."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzjyGIOHyu7V"
      },
      "source": [
        "### Supportive Functions for TIme Series Analysis :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gG_I0Cmbyu7V"
      },
      "outputs": [],
      "source": [
        "def test_stationarity(timeseries):\n",
        "    #Determing rolling statistics\n",
        "    MA = timeseries.rolling(window = 12).mean()\n",
        "    MSTD = timeseries.rolling(window = 12).std()\n",
        "\n",
        "    #Plot rolling statistics:\n",
        "    plt.figure(figsize=(15,5))\n",
        "    orig = plt.plot(timeseries, color='blue',label='Original')\n",
        "    mean = plt.plot(MA, color='red', label='Rolling Mean')\n",
        "    std = plt.plot(MSTD, color='black', label = 'Rolling Std')\n",
        "    plt.legend(loc='best')\n",
        "    plt.title('Rolling Mean & Standard Deviation')\n",
        "    plt.show(block=False)\n",
        "\n",
        "    #Perform Dickey-Fuller test:\n",
        "    print('Results of Dickey-Fuller Test:')\n",
        "    dftest = adfuller(timeseries, autolag='AIC')\n",
        "    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n",
        "    for key,value in dftest[4].items():\n",
        "        dfoutput['Critical Value (%s)'%key] = value\n",
        "    print(dfoutput)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3JjHMAoyyu7W"
      },
      "outputs": [],
      "source": [
        "def tsplot(y, lags=None, figsize=(12, 7), style='bmh'):\n",
        "    if not isinstance(y, pd.Series):\n",
        "        y = pd.Series(y)\n",
        "\n",
        "    with plt.style.context(style):\n",
        "        fig = plt.figure(figsize=figsize)\n",
        "        layout = (2, 2)\n",
        "        ts_ax = plt.subplot2grid(layout, (0, 0), colspan=2)\n",
        "        acf_ax = plt.subplot2grid(layout, (1, 0))\n",
        "        pacf_ax = plt.subplot2grid(layout, (1, 1))\n",
        "\n",
        "        y.plot(ax=ts_ax)\n",
        "        p_value = sm.tsa.stattools.adfuller(y)[1]\n",
        "        ts_ax.set_title('Time Series Analysis Plots\\n Dickey-Fuller: p={0:.5f}'.format(p_value))\n",
        "        smt.graphics.plot_acf(y, lags=lags, ax=acf_ax)\n",
        "        smt.graphics.plot_pacf(y, lags=lags, ax=pacf_ax)\n",
        "        plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-y19Mi5Wyu7W"
      },
      "source": [
        "### Components of Time Series :\n",
        "- A Time Series consists of the following components :\n",
        "\n",
        "    - **Trend** : Long term direction of the data.\n",
        "    \n",
        "    E.g : Year on year rising temperature of the Earth due to Global Warming.\n",
        "    \n",
        "    - **Seasonality** : Short term repetitve patterns of the data due to the weather seasons.\n",
        "    \n",
        "    E.g : Sale of sweaters specifically in the winter season.\n",
        "    \n",
        "    - **Cyclic Variations** : Short term repetitive patterns of the data over a period of 1 year.\n",
        "    \n",
        "    E.g : It usually consists of the Business Quarters i.e Q1, Q2, Q3 & Q4.\n",
        "    \n",
        "    - **Irregularities** : Random and unforseen fluctuations in the data.\n",
        "    \n",
        "    E.g : Occurrences of Earthquakes or Floods, etc.\n",
        "    \n",
        "    \n",
        "- **In order to assess a Time Series, we need to consider the above components and make sure that our data is free from all these components in order to make a forecast.**\n",
        "\n",
        "Let's visualize the **AveragePrice** data for the above components!\n",
        "- For this purpose, we use a function **seasonal_decompose** from the **statsmodel** library.\n",
        "- This function has a parameter, **model**, that needs to be assigned the value **additive** or **multiplicative**.\n",
        "    - **Additive Model** : Data has same width and height of the seasonal patterns or peaks. Trend of the data is linear.\n",
        "    - **Multiplicative Model** : Data has increasing / decreasing width and height of the seasonal patterns or peaks. Trend of the data is non-linear.\n",
        "\n",
        "From the visualizations of **AveragePrice** executed in EDA section, we can say that the data represents a **Multiplicative Model**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "03Ld1wztyu7W"
      },
      "outputs": [],
      "source": [
        "dec = sm.tsa.seasonal_decompose(df1['AveragePrice'], model = 'multiplicative').plot()\n",
        "plt.show();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2vPn4BRyu7W"
      },
      "source": [
        "- Data clearly has a **non-linear uptrend**.\n",
        "- A clear cut **seasonal** pattern is present in the data.\n",
        "- The last plot is the **Residual** plot. It is the plot that describes the data if the **trend** and **seasonal** components of the data are completely eliminated.\n",
        "- We also need to check the statistical parameters w.r.t time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJthfvxmyu7X"
      },
      "source": [
        "### Stationarity :\n",
        "\n",
        "- **Stationarity** is a concept for time series where statistical parameters like mean, variance, etc are all constant over time.\n",
        "- For a time series, features are dependent on time i.e **features are a function of time**. Statistical paramters will change values over time as they are extracted from the features.\n",
        "- If the time series is not stationary, then the predictions deviate from the original values and increase the error as we don't know the changes in these statistical parameters as they are a function of time.\n",
        "- Thus, by making time series stationary, we kind of nullify the effects of statistical parameters on the forecast.\n",
        "- To test stationarity, we will use **Augmented Dickey Fuller Test** :\n",
        "    - **Null Hypothesis** : It assumes that the time series is non-stationary.\n",
        "    - **Alternate Hypothesis** : If the null hypothesis is rejected, then the time series is stationary.\n",
        "    - Output of the **Augmented Dickey Fuller Test** include :\n",
        "        - **Test Statistic**\n",
        "        - **p-value**\n",
        "        - **#Lags Used**\n",
        "        - **Number of Observations Used**\n",
        "        - **Critical Value (1%)**\n",
        "        - **Critical Value (5%)**\n",
        "        - **Critical Value (10%)**\n",
        "- For the **Null Hypothesis** to be rejected and accepting that the time series is stationary, there are 2 requirements :\n",
        "    - **Critical Value (5%)** > **Test Statistic**\n",
        "    - **p-value** < 0.05"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Imn1czcIyu7X"
      },
      "outputs": [],
      "source": [
        "test_stationarity(df1['AveragePrice'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3qhb7s-yu7Y"
      },
      "source": [
        "- For the **AveragePrice** time series data,\n",
        "    - **Rolling Mean** is clearly variable with time. It is very close to the data. Thus, it can be a good descriptor of the data.\n",
        "    - **Rolling Standard Deviation** is pretty consistent in the initial stages however changes with time are observed in the later stages.\n",
        "    - **Test Statistic : (-2.36)** > **Critical Value (5%) : (-2.88)**\n",
        "    - **p-value (0.15)** > 0.05\n",
        "- Hence, **Null Hypothesis** cannot be rejected and we can conclude that the above **AveragePrice** time series is **not stationary**.\n",
        "- In order to eliminate trend, seasonality and make the time series stationary, we will use **differencing** i.e subtracting the previous value from it's next value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qv0JzsrAyu7Y"
      },
      "outputs": [],
      "source": [
        "df1['Log_AveragePrice'] = np.log(df1['AveragePrice'])\n",
        "df1_log_diff = df1['Log_AveragePrice'].diff()\n",
        "df1_log_diff = df1_log_diff.dropna()\n",
        "\n",
        "dec = sm.tsa.seasonal_decompose(df1_log_diff,period = 52).plot()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgGh7WMGyu7Z"
      },
      "source": [
        "- We have taken the **log** of the data to deal with **stationarity** and **differencing** is done to handle **trend** and **seasonality**.\n",
        "- **Trend** and **Seasonality** of the data have near about died down & their values have been reduced as well.\n",
        "- We now check the **stationarity** of the time series."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Q1QaxxDyu7a"
      },
      "outputs": [],
      "source": [
        "test_stationarity(df1_log_diff)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IA4uVKlqyu7a"
      },
      "source": [
        "- From the outputs of the **Augmented Dickey Fuller Test**,\n",
        "    - **Rolling Mean** is very close to 0 but very small variations are present in it.\n",
        "    - **Rolling Standard Deviation** is very close to 0.05 with some crests and troughs present throughout.\n",
        "    - **Critical Value (5%) : (-2.88)** > **Test Statistic : (-13.82)**. We can say that the time series is **stationary with 99%** confidence as the **Test Statistic** is less than **Critical Value (1%)**.\n",
        "    - **0.05** > **p-value (0.00)**\n",
        "- From these outputs, we can reject the **Null Hypothesis** and accept the **Alternate Hypothesis** i.e we can say that the above **time series is stationary**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmyq9_00yu7a"
      },
      "source": [
        "# <center>Modeling</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1P1NWPSyu7b"
      },
      "source": [
        "## <center>ARIMA</center>\n",
        "### <center>Auto Regressive Integrated Moving Average</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iu-LdwrKyu7b"
      },
      "source": [
        "- **ARIMA** model is a combination of 3 models :\n",
        "    - **AR (p) : Auto Regressive**\n",
        "    - **I (d) : Integrated**\n",
        "    - **MA (q) : Moving Average**\n",
        "\n",
        "- **(p,d,q)** is known as the order of the **ARIMA** model. Values of these parameters are based on the above mentioned models.  \n",
        "    - **p** : Number of auto regressive terms.\n",
        "    - **d** : Number of differencing orders required to make the time series stationary.\n",
        "    - **q** : Number of lagged forecast errors in the prediction equation.\n",
        "\n",
        "- Selection criteria for the order of **ARIMA** model :\n",
        "    - **p** : Lag value where the **Partial Autocorrelation (PACF)** graph cuts off or drops to 0 for the 1st instance.\n",
        "    - **d** : Number of times differencing is carried out to make the time series stationary.\n",
        "    - **q** : Lag value where the **Autocorrelation (ACF)** graph crosses the upper confidence interval for the 1st instance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tvmhNIuuyu7b"
      },
      "outputs": [],
      "source": [
        "tsplot(df1['AveragePrice'],lags = 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6I7rK3Eyu7c"
      },
      "source": [
        "- **ACF** graph provides the correlation between the time series & it's lags. For the above time series, we can observe a positive lowering correlation.\n",
        "- **PACF** graph provides the correlation between the time series and individual lags. These correlation coefficients are unlike the mutual correlations that are calculated in the presence of other features.\n",
        "- From the **PACF** graph above, the 1st lag is out of the confidence interval and probably the most significant lag. It probably dictates the pattern for the **ACF** graph where the next lag follows it's previous lag."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NLlLukydyu7c"
      },
      "outputs": [],
      "source": [
        "tsplot(df1_log_diff,lags = 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qiC-y04yu7c"
      },
      "source": [
        "- From the above plots, the following order of **ARIMA** model is selected from the selection criteria mentioned above :\n",
        "    - **p** : 1\n",
        "    - **d** : 1\n",
        "    - **q** : 2\n",
        "- The data passed for model fitting is the **Log_AveragePrice** data that is non-differenced data and non-stationary data.\n",
        "- In order to make the series stationary, we difference the time series. Thus, this differencing of data is carried out by the ARIMA model with the help of the **d** parameter that provides info about the order of differencing.\n",
        "- Thus, the parameters **p** and **q** are selected in such a way that we pass their values assuming the ARIMA model carries out the differencing process and makes the time series stationary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BA2Cox7Eyu7d"
      },
      "outputs": [],
      "source": [
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "\n",
        "model = ARIMA(df1['Log_AveragePrice'],order = (1,1,2))\n",
        "model_fit = model.fit()\n",
        "print(model_fit.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wkrl3jAnyu7d"
      },
      "source": [
        "- Model fitting is a measure of how well a machine learning model generalizes to similar data to that on which it was trained.\n",
        "- After fitting the ARIMA model, we now check this fit and compare this series with the original values of **AveragePrice**.\n",
        "- In this process, we store the model fitted values in a series. This series then undergoes the process of cummulative summation i.e opposite of differencing.\n",
        "- Result of cummulative summation is the expected log values fitted by the model. These log values are then exponentiated that creates the fitted original series."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_pIIW988yu7d"
      },
      "outputs": [],
      "source": [
        "# Fitted values of the model\n",
        "predictions_ARIMA = pd.Series(model_fit.fittedvalues, copy=True)\n",
        "\n",
        "# Reversing the 1st order Differencing by taking cumulative sum\n",
        "predictions_ARIMA_cumsum = predictions_ARIMA.cumsum()\n",
        "\n",
        "# Creating a series with the same length as the original 'Log_AveragePrice'\n",
        "# All the elements of this new series are same as the 1st element of the original 'Log_AveragePrice'\n",
        "expected_log_values = pd.Series(df1['Log_AveragePrice'].iloc[0], index = df1['Log_AveragePrice'].index)\n",
        "\n",
        "# Adding the elements of this newly created series with the cumulative sum series\n",
        "expected_log_values = expected_log_values.add(predictions_ARIMA_cumsum,fill_value = 0)\n",
        "\n",
        "# Taking log of the above series for getting the original values\n",
        "predictions_ARIMA_final = np.exp(expected_log_values)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnGZ42Ukyu7e"
      },
      "source": [
        "- RMSE value between **Fitted AveragePrice** and original **AveragePrice** is **21.83**.\n",
        "- This RMSE is slightly high and does creates the possibility of overfitting the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53uGh7Lgyu7e"
      },
      "source": [
        "### In - Sample Forecasting :\n",
        "\n",
        "- **In - Sample Forecasting** : Model forecasts values for the existing data points of the time series. It is similar to the train - test format for regression or classification problems.\n",
        "- We divide the data into train and test dataset. We reserve the last 30 elements for the test dataset and the remaining for the train dataset.\n",
        "- For this **In - Sample Forecasting**, we use the **rolling** forecast method i.e we predict or forecast a single value and use this predicted value again for model fitting for predicting the next value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qFblNCSRyu7e"
      },
      "outputs": [],
      "source": [
        "size = int(len(df1) - 30)\n",
        "train, test = df1['Log_AveragePrice'][0:size], df1['Log_AveragePrice'][size:len(df1)]\n",
        "\n",
        "print('\\t ARIMA MODEL : In - Sample Forecasting \\n')\n",
        "\n",
        "history = [x for x in train]\n",
        "predictions = []\n",
        "\n",
        "for t in range(len(test)):\n",
        "\n",
        "    model = ARIMA(history, order=(1,1,2))\n",
        "    model_fit = model.fit()\n",
        "\n",
        "    output = model_fit.forecast()\n",
        "    yhat = output[0]\n",
        "    predictions.append(float(yhat))\n",
        "\n",
        "    obs = test[t]\n",
        "    history.append(obs)\n",
        "\n",
        "    print('predicted = %f, expected = %f' % (np.exp(yhat), np.exp(obs)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3NDK9EPiyu7f"
      },
      "outputs": [],
      "source": [
        "predictions_series = pd.Series(predictions, index = test.index)\n",
        "fig,ax = plt.subplots(nrows = 1,ncols = 1,figsize = (15,5))\n",
        "\n",
        "plt.subplot(1,1,1)\n",
        "plt.plot(df1['AveragePrice'],label = 'Expected Values')\n",
        "plt.plot(np.exp(predictions_series),label = 'Predicted Values');\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzGGVV5myu7f"
      },
      "source": [
        "- **Predicted Values** and **Expected Values** are very close by and kind of display an overfit model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "alOoKn5Tyu7g"
      },
      "outputs": [],
      "source": [
        "error = np.sqrt(mean_squared_error(np.exp(test),np.exp(predictions)))\n",
        "print('Test RMSE: %.4f' % error)\n",
        "predictions_series = pd.Series(np.exp(predictions), index = test.index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mA0RchhHyu7g"
      },
      "source": [
        "- Test RMSE value is quite better than the fitted RMSE value.\n",
        "- Use of weekly data and presence of seasonality can also have an effect on the model performance. We will move on to **Out - Of - Sample Forecasting**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vE7C_jjFyu7g"
      },
      "source": [
        "### Out - of - Sample Forecasting :\n",
        "\n",
        "- **Out - of - Sample Forecasting** : Model forecasts values for the future data points by imputing the datetime index values of the time series.\n",
        "- We create a new dataframe with the future index values and same columns as the dataframe that we use for model fitting.\n",
        "- For this **Out - of - Sample Forecasting**, we use the **non-rolling** forecast method by using **forecast function** and **predict function**.\n",
        "- In the **non-rolling** method, we forecast or predict all future values at once."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9vFpgSJiyu7h"
      },
      "outputs": [],
      "source": [
        "from pandas.tseries.offsets import DateOffset\n",
        "future_dates = [df1.index[-1] + DateOffset(weeks = x) for x in range(0,52)]\n",
        "\n",
        "# New dataframe for storing the future values\n",
        "df2 = pd.DataFrame(index = future_dates[1:],columns = df1.columns)\n",
        "\n",
        "forecast = pd.concat([df1,df2])\n",
        "forecast['ARIMA_Forecast_Function'] = np.nan\n",
        "forecast['ARIMA_Predict_Function'] = np.nan\n",
        "forecast.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4g_h73hyu7i"
      },
      "source": [
        "- We create 2 new columns for storing the values forecasted or predicted into **ARIMA_Forecast_Function** and **ARIMA_Predict_Function**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBccfs4Qyu7j"
      },
      "source": [
        "#### Forecast Function :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AVPzt-_iyu7j"
      },
      "outputs": [],
      "source": [
        "f1 = np.array(np.exp(model_fit.forecast(steps = 51)))\n",
        "\n",
        "for i in range(len(f1)):\n",
        "    forecast.iloc[169 + i,2] = f1[i]\n",
        "forecast.tail()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0ioXWb3yu7k"
      },
      "source": [
        "- We use the **np.exp()** as we fit the model on log values and hence we unlog the values using exponentiation function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uU1mqnZ4yu7k"
      },
      "outputs": [],
      "source": [
        "forecast[['AveragePrice','ARIMA_Forecast_Function']].plot(figsize = (12,8));"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMgptB9Lyu7l"
      },
      "source": [
        "- Values generated by the **forecast_function** are constant."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4K0a5XL5yu7l"
      },
      "source": [
        "#### Predict Function :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9YtO591Gyu7m"
      },
      "outputs": [],
      "source": [
        "f2 = np.array(np.exp(model_fit.predict(start = 169,end = 219,typ = 'levels')))\n",
        "\n",
        "for i in range(51):\n",
        "    forecast.iloc[169 + i,3] = f2[i]\n",
        "forecast.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uo1ed0jlyu7m"
      },
      "outputs": [],
      "source": [
        "forecast[['AveragePrice','ARIMA_Predict_Function']].plot(figsize = (12,8));"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQa3eX17yu7m"
      },
      "source": [
        "- Similar pattern can be observed for the values generated by the **predict_function**.\n",
        "- Let's compare the values generated by the **forecast_function** and **predict_function**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JiBzD8Evyu7n"
      },
      "outputs": [],
      "source": [
        "sum(f1) == sum(f2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbVBDQIWyu7o"
      },
      "source": [
        "- For the above trained ARIMA model, values generated by **forecast_function** and **predict function** are either exactly identical or just differ by a few decimal points!\n",
        "- The model clearly did not capture the **seasonal patterns** of the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-D3fM2UByu7o"
      },
      "source": [
        "## <center>SARIMA</center>\n",
        "### <center>Seasonal Auto Regressive Integrated Moving Average</center>\n",
        "\n",
        "- **SARIMA** model is an extension of the ARIMA model that can handle the seasonal effects of the data.\n",
        "- It has kind of 2 orders **(p,d,q) x (P,D,Q,M)**.\n",
        "- **(p,d,q)** is the order that is similar to the order of the **ARIMA** model.\n",
        "- **(P,D,Q,M)** is known as the Seasonal Order where **(P,D,Q)** are similar to the **(p,d,q)** of the ARIMA model.\n",
        "- It's selection criteria is similar as well with an important condition i.e to handle the seasonality by differencing the data with the frequency of seasonal period or periodicity, **M**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tlSwyjw4yu7p"
      },
      "outputs": [],
      "source": [
        "df1_log_diff_seas = df1_log_diff.diff(52)\n",
        "df1_log_diff_seas = df1_log_diff_seas.dropna()\n",
        "dec = sm.tsa.seasonal_decompose(df1_log_diff_seas).plot()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGvmKak5yu7p"
      },
      "source": [
        "- For our data, it is in **weekly format** and the **seasonal period is of 1 year**.\n",
        "- Hence, we difference the already differenced data by a periodicity, **M**, value of 52.\n",
        "- We can observe that the seasonality of the data nearabout gone with y-axis values ranging from -0.1 to 0.1.\n",
        "- We will check this seasonal differenced data for stationarity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sVdVT9xvyu7q"
      },
      "outputs": [],
      "source": [
        "test_stationarity(df1_log_diff_seas)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FI_QfnLDyu72"
      },
      "source": [
        "- From the outputs of the **Augmented Dickey Fuller Test**,\n",
        "    - **Rolling Mean** is very close to 0 but very small variations are present in it.\n",
        "    - **Rolling Standard Deviation** is very close to 0.05 with some crests and troughs present throughout.\n",
        "    - **Critical Value (5%) : (-2.89)** > **Test Statistic : (-4.60)**. We can say that the time series is **stationary with 99%** confidence as the **Test Statistic** is less than **Critical Value (1%)** as well.\n",
        "    - **0.05** > **p-value (0.00)**\n",
        "- From these outputs, we can reject the **Null Hypothesis** and accept the **Alternate Hypothesis** i.e we can say that the above **time series is stationary**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H-5XdbgDyu73"
      },
      "outputs": [],
      "source": [
        "tsplot(df1_log_diff_seas)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Sx4rcAOyu73"
      },
      "source": [
        "- For the **SARIMA** model, order of **(p,d,q)** will be same as the order of the **ARIMA** model above.\n",
        "- For **(P,D,Q,M)** :\n",
        "    - **P** : 0\n",
        "    - **D** : 1\n",
        "    - **Q** : 0\n",
        "    - **M** : 52\n",
        "- For **SARIMA**, creating a grid search framework and selecting the parameters with the lowest **AIC** & **BIC** values is also a popular choice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SYVfDBC9yu74"
      },
      "outputs": [],
      "source": [
        "model = sm.tsa.statespace.SARIMAX(df1['Log_AveragePrice'],order = (1,1,2),seasonal_order = (0,1,0,52))\n",
        "model_fit = model.fit()\n",
        "print(model_fit.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8hMr419yu75"
      },
      "source": [
        "### In - Sample Forecasting :\n",
        "\n",
        "- **In - Sample Forecasting** : Model forecasts values for the existing data points of the time series. It is similar to the train - test format for regression or classification problems.\n",
        "- We divide the data into train and test dataset. We reserve the last 30 elements for the test dataset and the remaining for the train dataset similar to the approach of **ARIMA** model.\n",
        "- For this **In - Sample Forecasting**, we use the **rolling** forecast method i.e we predict or forecast a single value and use this predicted value again for model fitting for predicting the next value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ui4KLhN1yu75"
      },
      "outputs": [],
      "source": [
        "size = int(len(df1) - 30)\n",
        "train, test = df1['Log_AveragePrice'][0:size], df1['Log_AveragePrice'][size:len(df1)]\n",
        "\n",
        "print('\\t SARIMA MODEL : In - Sample Forecasting \\n')\n",
        "\n",
        "history = [x for x in train]\n",
        "predictions = []\n",
        "\n",
        "for t in range(len(test)):\n",
        "\n",
        "    model = sm.tsa.statespace.SARIMAX(history,order = (1,1,2),seasonal_order = (0,1,0,52))\n",
        "    model_fit = model.fit(disp = 0)\n",
        "\n",
        "    output = model_fit.forecast()\n",
        "\n",
        "    yhat = output[0]\n",
        "    predictions.append(float(yhat))\n",
        "\n",
        "    obs = test[t]\n",
        "    history.append(obs)\n",
        "\n",
        "    print('predicted = %f, expected = %f' % (np.exp(yhat), np.exp(obs)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d5TgpxK-yu76"
      },
      "outputs": [],
      "source": [
        "predictions_series = pd.Series(predictions, index = test.index)\n",
        "fig,ax = plt.subplots(nrows = 1,ncols = 1,figsize = (15,5))\n",
        "\n",
        "plt.subplot(1,1,1)\n",
        "plt.plot(df1['AveragePrice'],label = 'Expected Values')\n",
        "plt.plot(np.exp(predictions_series),label = 'Predicted Values');\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEmtfNyoyu76"
      },
      "source": [
        "- **Predicted Values** and **Expected Values** are very close by and kind of display an overfit model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ka7jFFimyu76"
      },
      "outputs": [],
      "source": [
        "error = np.sqrt(mean_squared_error(np.exp(test),np.exp(predictions)))\n",
        "print('Test RMSE: %.4f' % error)\n",
        "predictions_series = pd.Series(np.exp(predictions), index = test.index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePUoJCCeyu76"
      },
      "source": [
        "- Test RMSE value is ok.\n",
        "- We will move on to **Out - Of - Sample Forecasting**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfMDnU4Ayu77"
      },
      "source": [
        "### Out of Sample Forecasting :\n",
        "\n",
        "- **Out - of - Sample Forecasting** : Model forecasts values for the future data points by imputing the datetime index values of the time series.\n",
        "- For this **Out - of - Sample Forecasting**, we use the **non-rolling** forecast method by using **forecast function** and **predict function**.\n",
        "- In the **non-rolling** method, we forecast or predict all future values at once. We store these future values into 2 new columns **SARIMA_Forecast_Function** and **SARIMA_Predict_Function** in the existing **forecast** dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GaZXa0buyu77"
      },
      "outputs": [],
      "source": [
        "forecast['SARIMA_Forecast_Function'] = np.nan\n",
        "forecast['SARIMA_Predict_Function'] = np.nan\n",
        "forecast.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGd8OPFeyu77"
      },
      "source": [
        "#### Forecast Function :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imVYYlaJyu77"
      },
      "outputs": [],
      "source": [
        "f3 = np.array(np.exp(model_fit.forecast(steps = 51)))\n",
        "\n",
        "for i in range(len(f3)):\n",
        "    forecast.iloc[169 + i,4] = f3[i]\n",
        "forecast.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ranr_Spwyu77"
      },
      "outputs": [],
      "source": [
        "forecast[['AveragePrice','SARIMA_Forecast_Function']].plot(figsize = (12,8));"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJNMb_Siyu77"
      },
      "source": [
        "- From the above graph, we can say that the **SARIMA** model was able to capture the seasonal patterns and created a slight uptrend future predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKn9a4T6yu78"
      },
      "source": [
        "#### Predict Function :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A5xt_3svyu78"
      },
      "outputs": [],
      "source": [
        "f4 = np.array(np.exp(model_fit.predict(start = 169,end = 219,typ = 'levels')))\n",
        "\n",
        "for i in range(51):\n",
        "    forecast.iloc[169 + i,5] = f4[i]\n",
        "forecast.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ey7SOnayu78"
      },
      "outputs": [],
      "source": [
        "forecast[['AveragePrice','SARIMA_Predict_Function']].plot(figsize = (12,8));"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xz3BaYPyu78"
      },
      "source": [
        "- The graph is very similar to the graph above, we will check the values of **SARIMA_Forecast_Function** and **SARIMA_Predict_Function**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AZZys2bUyu78"
      },
      "outputs": [],
      "source": [
        "sum(f3) == sum(f4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NztasKyDyu79"
      },
      "source": [
        "- For the above trained **SARIMA** model, values generated by **forecast_function** and **predict function** are either exactly identical or just differ by a few decimal points!\n",
        "- This model captured the **seasonal patterns** of the data much better!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHhmtHWSyu79"
      },
      "source": [
        "# <center>Conclusion</center>\n",
        "\n",
        "- This is a good dataset for understanding **Time Series Analysis** problems. It opens up the opportunity to learn multiple concepts and dealing with weekly data.\n",
        "\n",
        "\n",
        "- EDA section provides some great insights that can be associated with the business problems for selling of avocados.\n",
        "\n",
        "\n",
        "- For the **Time Series Analysis** section, stationarity, understanding the kind of data the models train on, different transforms that are carried out, etc can be a tricky and abit overwhelming as well. This is because it has too many things that need to be dealt with.  \n",
        "\n",
        "\n",
        "- Difference between **ARIMA** and **SARIMA** model can also be clearly understood. Similarly, usage of **forecast_function** & **predict_function** for **In - Sample Forecasting** & **Out - of - Sample Forecasting** using **rolling** and **non - rolling** method can be understood.  "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}